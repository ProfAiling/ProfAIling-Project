{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries.\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spacy import tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tkz = tokenizer.Tokenizer(nlp.vocab)\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import unicodedata\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/train_clean_2.pkl')\n",
    "\n",
    "train = pd.read_json('/Users/simonefacchiano/Desktop/Data Science/SL/Project/SL-Final-Project/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_class'] = pd.cut(\n",
    "        train[\"age\"],\n",
    "        bins=[12, 18, 28, 50],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(\"int\")\n",
    "\n",
    "# Possiamo quindi rimuovere la variabile age originale\n",
    "train = train.drop('age', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean['age_class'] = pd.cut(\n",
    "        train_clean[\"age\"],\n",
    "        bins=[12, 18, 28, 50],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(\"int\")\n",
    "\n",
    "# Possiamo quindi rimuovere la variabile age originale\n",
    "train = train_clean.drop('age', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['word_count'] = train['post'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.insert(1, 'clean_post', train_clean.post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuizione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idea è la seguente. Noi per il momento abbiamo solo considerato il contenuto delle frasi. Ci siamo riusciti facendo un pre-processing bello corposo, e poi facendo l'embedding dei post per avere una rappresentazione numerica, diventata l'unica feature all'interno del nostro primissimo modello logistico (0.64 di accuracy).\n",
    "\n",
    "Tuttavia, non stiamo considerando altri fattori. Ad esempio, potrebbero esserci indicazioni importanti dalla punteggiatura, dalle emoticon, dai link o dell'utilizzo di lettere maiuscole e minuscole, oltre ovviamente alla lunghezza dei post.\n",
    "Per questo motivo, proviamo ora a vedere come si distribuiscono questi elementi tra i due generi.\n",
    "\n",
    "In un secondo momento, ripeteremo questa stessa analisi anche per le classi di età."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: gender, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['post'].str.contains(\":\\)\", regex=True)].gender.value_counts()\n",
    "\n",
    "# 12mila vs 7mila... e non è l'unico esempio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creiamo una lista di possibili emoticon:\n",
    "emoticons = [r':\\)', r':-\\)', r':\\(', r':-\\(', r';\\)', r';-\\)', r':D', r':-D', r':P', r':-P', r':O', r':-O', r':\\|', r':-\\|', r'>:\\(', r\":'\\(\", r\":'-\\(\", r'XD', r'<3', r':3', r'>:-0']\n",
    "\n",
    "# Cerchiamo:\n",
    "len( train[train['post'].str.contains('|'.join(emoticons), regex=True)] )\n",
    "# 55mila righe... più del 10% dell'intero dataset contiene queste emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: gender, dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Le emoticon possono discriminare il genere...\n",
    "train[train['post'].str.contains('|'.join(emoticons), regex=True)].gender.value_counts() # c'è una evidente differenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: age_class, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... ma soprattutto possono discriminare l'età\n",
    "train[train['post'].str.contains('|'.join(emoticons), regex=True)].age_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creiamo la variabile \"has_emoticon\"\n",
    "train['has_emoticon'] = train['post'].str.contains('|'.join(emoticons), regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    239520\n",
       "0    169678\n",
       "2     94699\n",
       "Name: age_class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C'è però un problema da tenere in considerazione: \n",
    "# è vero, a usare le emoticon sono soprattuto i giovani... ma poiché ci sono meno persone che della seconda classe d'età abbiamo che:\n",
    "train[train.has_emoticon == 0].age_class.value_counts()\n",
    "\n",
    "# A *non* usare le emoticon sono nuovamente i giovani!\n",
    "# Dovremo risolvere questo inconveniente usando dei pesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punteggiatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "characters = ['...', '!!', '??', '?!']\n",
    "pattern = '|'.join([re.escape(char) for char in characters])\n",
    "\n",
    "len(train[train['post'].str.contains(pattern)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forse non cambia troppo con il genere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: gender, dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['post'].str.contains(pattern)].gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma con l'età scopriamo una cosa interessante..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: age_class, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['post'].str.contains(pattern)].age_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['has_punctuation'] = train['post'].str.contains(pattern).astype(int)\n",
    "\n",
    "# Ma di nuovo, serviranno probabilmente dei pesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabolario e link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'idea è quella di sfruttare la presenza di parole che siano \"specifiche\" di una certa classe (di età o di genere).\n",
    "# Per farlo usiamo la seguente funzione:\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def most_common_words(dataset):\n",
    "    #stop = set(stopwords.words(\"english\"))\n",
    "    corpus = [word for i in dataset[\"clean_post\"].str.split().values.tolist() for word in i] # if (word not in stop)\n",
    "    most_common = FreqDist(corpus).most_common()\n",
    "    words, frequency = [], []\n",
    "    for word, count in most_common:\n",
    "        words.append(word)\n",
    "        frequency.append(count)   \n",
    "    return dict(zip(words, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Con queste ci ho provato ma non migliorano il modello\n",
    "\n",
    "# Estraiamo poi le parole più comuni per ogni gruppo\n",
    "\n",
    "girl_dict = most_common_words(train[train.gender == 'female'])\n",
    "boy_dict = most_common_words(train[train.gender == 'male'])\n",
    "\n",
    "young_dict = most_common_words(train[train.age_class == 0])\n",
    "medium_dict = most_common_words(train[train.age_class == 1])\n",
    "old_dict = most_common_words(train[train.age_class == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ricopio quello che dice il paper\n",
    "\n",
    "girl_words = [word for word, freq in girl_dict.items() if freq > 5000]\n",
    "boy_words = [word for word, freq in boy_dict.items() if freq > 5000]\n",
    "\n",
    "young_words = [word for word, freq in young_dict.items() if freq > 5000]\n",
    "medium_words = [word for word, freq in medium_dict.items() if freq > 5000]\n",
    "old_words = [word for word, freq in old_dict.items() if freq > 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E infine facciamo una differenza tra i set di parole, in modo da ottenere parole molto \"specifiche\" per un certo sottogruppo di autori\n",
    "\n",
    "# Ma anche questo funzionava poco. Seguiamo cosa dice il paper\n",
    "\n",
    "only_girls = list(set(girl_words) - set(boy_words))\n",
    "only_boys = list(set(boy_words) - set(girl_words))\n",
    "\n",
    "only_young = list(set(young_words) - set(medium_words) - set(old_words))\n",
    "only_medium = list(set(medium_words) - set(young_words) - set(old_words))\n",
    "only_old = list(set(old_words) - set(medium_words) - set(young_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il paper propone la seguente cosa:\n",
    "\n",
    "# si prendono le parole che hanno una frequenza > 5000\n",
    "# si vedono quelle parole che hanno una differenza di frequenza maggiore tra i vari gruppi\n",
    "# e si considerano quelle come feature\n",
    "\n",
    "# Ad esempio, la parola \"football\" comparirà con una frequenza molto grande per i maschi e molto piccola per le femmine, quindi la parola \"football\" è in qualche modo discriminante.\n",
    "# Proviamo a farlo\n",
    "\n",
    "#                  ----> #### GENDER #### <----\n",
    "\n",
    "dict_boys_vs_girls = dict()\n",
    "\n",
    "for word in boy_words:\n",
    "    freq_m = boy_dict[word]\n",
    "\n",
    "    try:\n",
    "        freq_f = girl_dict[word]\n",
    "    except KeyError:\n",
    "        freq_f = 0\n",
    "\n",
    "    dict_boys_vs_girls[word] = freq_m - freq_f # se > 0 compare più nei maschi, se < 0 compare di più nelle femmine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 12127),\n",
       " ('bush', 11679),\n",
       " ('war', 9728),\n",
       " ('post', 9506),\n",
       " ('site', 7111),\n",
       " ('government', 6777),\n",
       " ('president', 6465),\n",
       " ('blog', 6404),\n",
       " ('news', 6385),\n",
       " ('use', 6371),\n",
       " ('iraq', 6147),\n",
       " ('american', 6087),\n",
       " ('system', 5898),\n",
       " ('kerry', 5886),\n",
       " ('play', 5834),\n",
       " ('games', 5694),\n",
       " ('team', 5687),\n",
       " ('com', 5664),\n",
       " ('america', 5180),\n",
       " ('country', 5016),\n",
       " ('state', 4909),\n",
       " ('web', 4819),\n",
       " ('states', 4819),\n",
       " ('point', 4683),\n",
       " ('music', 4661),\n",
       " ('information', 4637),\n",
       " ('free', 4606),\n",
       " ('article', 4454),\n",
       " ('film', 4351),\n",
       " ('gay', 4338),\n",
       " ('united', 4155),\n",
       " ('playing', 4152),\n",
       " ('played', 4107),\n",
       " ('political', 4025),\n",
       " ('media', 3931),\n",
       " ('law', 3896),\n",
       " ('win', 3855),\n",
       " ('power', 3769),\n",
       " ('s', 3755),\n",
       " ('movie', 3721),\n",
       " ('problem', 3690),\n",
       " ('based', 3678),\n",
       " ('national', 3676),\n",
       " ('public', 3590),\n",
       " ('internet', 3370),\n",
       " ('business', 3252),\n",
       " ('video', 3233),\n",
       " ('page', 3226),\n",
       " ('support', 3210),\n",
       " ('al', 3198),\n",
       " ('link', 3187),\n",
       " ('john', 3176),\n",
       " ('album', 3171),\n",
       " ('case', 3141),\n",
       " ('question', 3132),\n",
       " ('service', 3130),\n",
       " ('group', 3113),\n",
       " ('mr', 3089),\n",
       " ('human', 3086),\n",
       " ('set', 3077),\n",
       " ('security', 3040),\n",
       " ('end', 2995),\n",
       " ('fact', 2992),\n",
       " ('americans', 2975),\n",
       " ('military', 2966),\n",
       " ('action', 2959),\n",
       " ('history', 2927),\n",
       " ('example', 2918),\n",
       " ('computer', 2879),\n",
       " ('interesting', 2797),\n",
       " ('society', 2746),\n",
       " ('million', 2741),\n",
       " ('series', 2727),\n",
       " ('future', 2678),\n",
       " ('city', 2634),\n",
       " ('second', 2632),\n",
       " ('vote', 2593),\n",
       " ('issue', 2584),\n",
       " ('number', 2547),\n",
       " ('money', 2537),\n",
       " ('story', 2534),\n",
       " ('level', 2496),\n",
       " ('paul', 2496),\n",
       " ('simply', 2476),\n",
       " ('form', 2406),\n",
       " ('probably', 2356),\n",
       " ('record', 2335),\n",
       " ('report', 2317),\n",
       " ('local', 2294),\n",
       " ('company', 2285),\n",
       " ('general', 2270),\n",
       " ('version', 2267),\n",
       " ('season', 2254),\n",
       " ('final', 2224),\n",
       " ('pm', 2210),\n",
       " ('real', 2195),\n",
       " ('posts', 2189),\n",
       " ('george', 2186),\n",
       " ('click', 2181),\n",
       " ('star', 2176),\n",
       " ('community', 2170),\n",
       " ('simple', 2150),\n",
       " ('wife', 2144),\n",
       " ('create', 2143),\n",
       " ('according', 2124),\n",
       " ('search', 2106),\n",
       " ('title', 2059),\n",
       " ('songs', 2050),\n",
       " ('following', 2043),\n",
       " ('bill', 2031),\n",
       " ('website', 1991),\n",
       " ('years', 1987),\n",
       " ('main', 1981),\n",
       " ('recently', 1980),\n",
       " ('program', 1977),\n",
       " ('important', 1975),\n",
       " ('force', 1970),\n",
       " ('current', 1964),\n",
       " ('line', 1935),\n",
       " ('project', 1934),\n",
       " ('view', 1933),\n",
       " ('process', 1911),\n",
       " ('points', 1898),\n",
       " ('far', 1890),\n",
       " ('rock', 1890),\n",
       " ('possible', 1875),\n",
       " ('ideas', 1875),\n",
       " ('michael', 1853),\n",
       " ('lead', 1850),\n",
       " ('band', 1843),\n",
       " ('major', 1839),\n",
       " ('order', 1837),\n",
       " ('death', 1827),\n",
       " ('given', 1826),\n",
       " ('peace', 1818),\n",
       " ('shot', 1804),\n",
       " ('students', 1791),\n",
       " ('members', 1771),\n",
       " ('won', 1758),\n",
       " ('york', 1724),\n",
       " ('including', 1723),\n",
       " ('issues', 1699),\n",
       " ('experience', 1694),\n",
       " ('large', 1691),\n",
       " ('space', 1679),\n",
       " ('shows', 1676),\n",
       " ('likely', 1676),\n",
       " ('run', 1666),\n",
       " ('cd', 1577),\n",
       " ('basically', 1558),\n",
       " ('fuck', 1553),\n",
       " ('chinese', 1541),\n",
       " ('beat', 1536),\n",
       " ('check', 1535),\n",
       " ('social', 1533),\n",
       " ('certainly', 1514),\n",
       " ('update', 1508),\n",
       " ('radio', 1505),\n",
       " ('act', 1496),\n",
       " ('mail', 1488),\n",
       " ('jesus', 1477),\n",
       " ('idea', 1447),\n",
       " ('seen', 1446),\n",
       " ('hit', 1404),\n",
       " ('evil', 1394),\n",
       " ('road', 1391),\n",
       " ('marriage', 1361),\n",
       " ('problems', 1360),\n",
       " ('bus', 1338),\n",
       " ('style', 1297),\n",
       " ('means', 1288),\n",
       " ('personal', 1284),\n",
       " ('tv', 1282),\n",
       " ('couple', 1262),\n",
       " ('note', 1262),\n",
       " ('message', 1254),\n",
       " ('online', 1239),\n",
       " ('continue', 1227),\n",
       " ('cool', 1221),\n",
       " ('chance', 1202),\n",
       " ('clear', 1200),\n",
       " ('small', 1192),\n",
       " ('easy', 1189),\n",
       " ('worth', 1171),\n",
       " ('building', 1166),\n",
       " ('currently', 1145),\n",
       " ('control', 1144),\n",
       " ('ball', 1140),\n",
       " ('posted', 1133),\n",
       " ('fucking', 1130),\n",
       " ('piece', 1110),\n",
       " ('sound', 1106),\n",
       " ('reason', 1104),\n",
       " ('round', 1089),\n",
       " ('church', 1081),\n",
       " ('daily', 1078),\n",
       " ('short', 1076),\n",
       " ('return', 1073),\n",
       " ('pretty', 1057),\n",
       " ('looks', 1045),\n",
       " ('earth', 1040),\n",
       " ('f', 1018),\n",
       " ('air', 1005),\n",
       " ('quick', 1002),\n",
       " ('fire', 994),\n",
       " ('truth', 981),\n",
       " ('questions', 980),\n",
       " ('follow', 980),\n",
       " ('character', 975),\n",
       " ('takes', 969),\n",
       " ('expect', 958),\n",
       " ('near', 943),\n",
       " ('area', 940),\n",
       " ('etc', 897),\n",
       " ('fight', 894),\n",
       " ('known', 892),\n",
       " ('kill', 892),\n",
       " ('earlier', 892),\n",
       " ('dead', 886),\n",
       " ('english', 880),\n",
       " ('word', 874),\n",
       " ('lack', 874),\n",
       " ('pay', 872),\n",
       " ('complete', 865),\n",
       " ('reasons', 861),\n",
       " ('gives', 844),\n",
       " ('eventually', 842),\n",
       " ('different', 833),\n",
       " ('list', 823),\n",
       " ('works', 819),\n",
       " ('answer', 810),\n",
       " ('ground', 806),\n",
       " ('open', 791),\n",
       " ('beer', 786),\n",
       " ('everybody', 784),\n",
       " ('study', 780),\n",
       " ('managed', 752),\n",
       " ('interested', 746),\n",
       " ('read', 740),\n",
       " ('test', 733),\n",
       " ('apparently', 732),\n",
       " ('train', 732),\n",
       " ('box', 715),\n",
       " ('reality', 702),\n",
       " ('comment', 698),\n",
       " ('choice', 692),\n",
       " ('shit', 686),\n",
       " ('non', 679),\n",
       " ('high', 671),\n",
       " ('single', 662),\n",
       " ('movies', 655),\n",
       " ('enjoy', 653),\n",
       " ('young', 653),\n",
       " ('began', 650),\n",
       " ('comes', 638),\n",
       " ('step', 637),\n",
       " ('practice', 632),\n",
       " ('start', 631),\n",
       " ('certain', 628),\n",
       " ('plan', 619),\n",
       " ('place', 579),\n",
       " ('ways', 577),\n",
       " ('add', 539),\n",
       " ('quickly', 536),\n",
       " ('straight', 532),\n",
       " ('taken', 527),\n",
       " ('comments', 507),\n",
       " ('turns', 506),\n",
       " ('shall', 502),\n",
       " ('street', 487),\n",
       " ('noticed', 463),\n",
       " ('low', 457),\n",
       " ('later', 441),\n",
       " ('sense', 431),\n",
       " ('total', 430),\n",
       " ('written', 414),\n",
       " ('speaking', 409),\n",
       " ('fast', 386),\n",
       " ('bunch', 377),\n",
       " ('present', 346),\n",
       " ('send', 338),\n",
       " ('pass', 331),\n",
       " ('true', 307),\n",
       " ('park', 304),\n",
       " ('entire', 295),\n",
       " ('k', 291),\n",
       " ('needs', 283),\n",
       " ('running', 281),\n",
       " ('usual', 269),\n",
       " ('big', 263),\n",
       " ('worst', 259),\n",
       " ('type', 257),\n",
       " ('crap', 242),\n",
       " ('black', 234),\n",
       " ('period', 230),\n",
       " ('song', 225),\n",
       " ('deal', 217),\n",
       " ('save', 213),\n",
       " ('passed', 208),\n",
       " ('starts', 203),\n",
       " ('best', 201),\n",
       " ('situation', 185),\n",
       " ('instead', 182),\n",
       " ('office', 179),\n",
       " ('listen', 174),\n",
       " ('father', 170),\n",
       " ('king', 166),\n",
       " ('men', 165),\n",
       " ('heard', 157),\n",
       " ('hand', 152),\n",
       " ('attention', 142),\n",
       " ('mention', 142),\n",
       " ('past', 138),\n",
       " ('beginning', 135),\n",
       " ('stories', 133),\n",
       " ('middle', 132),\n",
       " ('email', 117),\n",
       " ('card', 108),\n",
       " ('course', 101),\n",
       " ('white', 101),\n",
       " ('forward', 98),\n",
       " ('changed', 79),\n",
       " ('gets', 71),\n",
       " ('ride', 66),\n",
       " ('club', 57),\n",
       " ('places', 54),\n",
       " ('rest', 44),\n",
       " ('bit', 28),\n",
       " ('special', 24),\n",
       " ('saying', 23),\n",
       " ('matter', 10),\n",
       " ('lives', 7),\n",
       " ('table', 6),\n",
       " ('worse', 4),\n",
       " ('boring', -17),\n",
       " ('soon', -28),\n",
       " ('working', -29),\n",
       " ('art', -30),\n",
       " ('class', -33),\n",
       " ('half', -53),\n",
       " ('drive', -64),\n",
       " ('finished', -69),\n",
       " ('giving', -69),\n",
       " ('catch', -71),\n",
       " ('speak', -73),\n",
       " ('hopefully', -78),\n",
       " ('c', -81),\n",
       " ('damn', -86),\n",
       " ('change', -92),\n",
       " ('bar', -95),\n",
       " ('chris', -99),\n",
       " ('especially', -100),\n",
       " ('buy', -109),\n",
       " ('writing', -125),\n",
       " ('caught', -128),\n",
       " ('late', -129),\n",
       " ('fear', -148),\n",
       " ('imagine', -149),\n",
       " ('truly', -153),\n",
       " ('strong', -156),\n",
       " ('ass', -161),\n",
       " ('paper', -173),\n",
       " ('cut', -176),\n",
       " ('longer', -182),\n",
       " ('college', -200),\n",
       " ('goes', -213),\n",
       " ('sounds', -221),\n",
       " ('meeting', -224),\n",
       " ('light', -243),\n",
       " ('able', -255),\n",
       " ('starting', -258),\n",
       " ('happens', -259),\n",
       " ('words', -261),\n",
       " ('evening', -261),\n",
       " ('minute', -270),\n",
       " ('july', -270),\n",
       " ('minutes', -271),\n",
       " ('sort', -272),\n",
       " ('bring', -273),\n",
       " ('poor', -290),\n",
       " ('l', -290),\n",
       " ('la', -318),\n",
       " ('normal', -323),\n",
       " ('hell', -334),\n",
       " ('found', -339),\n",
       " ('thoughts', -343),\n",
       " ('self', -363),\n",
       " ('ago', -372),\n",
       " ('learn', -381),\n",
       " ('blood', -382),\n",
       " ('sign', -388),\n",
       " ('kid', -409),\n",
       " ('g', -425),\n",
       " ('teacher', -429),\n",
       " ('reading', -432),\n",
       " ('lose', -437),\n",
       " ('exactly', -444),\n",
       " ('turn', -450),\n",
       " ('times', -458),\n",
       " ('die', -460),\n",
       " ('conversation', -466),\n",
       " ('p', -470),\n",
       " ('finish', -471),\n",
       " ('wednesday', -481),\n",
       " ('early', -488),\n",
       " ('mind', -492),\n",
       " ('random', -493),\n",
       " ('trip', -512),\n",
       " ('share', -523),\n",
       " ('sent', -536),\n",
       " ('super', -540),\n",
       " ('eye', -546),\n",
       " ('moment', -555),\n",
       " ('listening', -559),\n",
       " ('stopped', -559),\n",
       " ('age', -561),\n",
       " ('party', -566),\n",
       " ('stand', -572),\n",
       " ('plus', -572),\n",
       " ('meant', -573),\n",
       " ('town', -587),\n",
       " ('e', -589),\n",
       " ('son', -607),\n",
       " ('happened', -623),\n",
       " ('alot', -623),\n",
       " ('ah', -625),\n",
       " ('x', -628),\n",
       " ('month', -633),\n",
       " ('sun', -656),\n",
       " ('n', -660),\n",
       " ('thursday', -677),\n",
       " ('asking', -685),\n",
       " ('worked', -695),\n",
       " ('hold', -703),\n",
       " ('picked', -711),\n",
       " ('turned', -713),\n",
       " ('wrote', -737),\n",
       " ('making', -738),\n",
       " ('wrong', -766),\n",
       " ('hands', -766),\n",
       " ('picture', -772),\n",
       " ('coming', -796),\n",
       " ('dont', -800),\n",
       " ('ran', -801),\n",
       " ('sucks', -808),\n",
       " ('till', -814),\n",
       " ('de', -816),\n",
       " ('spent', -824),\n",
       " ('r', -826),\n",
       " ('ended', -838),\n",
       " ('voice', -846),\n",
       " ('guy', -856),\n",
       " ('suppose', -860),\n",
       " ('happen', -867),\n",
       " ('deep', -887),\n",
       " ('hmm', -888),\n",
       " ('ha', -909),\n",
       " ('usually', -926),\n",
       " ('lost', -931),\n",
       " ('strange', -939),\n",
       " ('st', -948),\n",
       " ('ill', -974),\n",
       " ('believe', -977),\n",
       " ('moving', -993),\n",
       " ('moved', -997),\n",
       " ('feet', -1000),\n",
       " ('looking', -1002),\n",
       " ('pick', -1003),\n",
       " ('spend', -1009),\n",
       " ('driving', -1017),\n",
       " ('wake', -1020),\n",
       " ('book', -1027),\n",
       " ('lord', -1028),\n",
       " ('green', -1029),\n",
       " ('visit', -1039),\n",
       " ('seeing', -1048),\n",
       " ('months', -1054),\n",
       " ('understand', -1059),\n",
       " ('floor', -1059),\n",
       " ('tuesday', -1061),\n",
       " ('close', -1064),\n",
       " ('taking', -1077),\n",
       " ('meet', -1078),\n",
       " ('huge', -1083),\n",
       " ('old', -1089),\n",
       " ('fat', -1093),\n",
       " ('date', -1097),\n",
       " ('learned', -1098),\n",
       " ('fine', -1101),\n",
       " ('ones', -1138),\n",
       " ('shirt', -1152),\n",
       " ('rain', -1153),\n",
       " ('hour', -1194),\n",
       " ('afternoon', -1244),\n",
       " ('missed', -1244),\n",
       " ('drink', -1271),\n",
       " ('liked', -1301),\n",
       " ('blue', -1303),\n",
       " ('outside', -1313),\n",
       " ('started', -1319),\n",
       " ('live', -1352),\n",
       " ('living', -1354),\n",
       " ('dark', -1358),\n",
       " ('left', -1372),\n",
       " ('walking', -1380),\n",
       " ('amazing', -1396),\n",
       " ('thats', -1418),\n",
       " ('figure', -1424),\n",
       " ('forget', -1442),\n",
       " ('leaving', -1442),\n",
       " ('red', -1447),\n",
       " ('stuff', -1452),\n",
       " ('try', -1452),\n",
       " ('definitely', -1453),\n",
       " ('completely', -1458),\n",
       " ('watching', -1477),\n",
       " ('finally', -1478),\n",
       " ('break', -1488),\n",
       " ('monday', -1528),\n",
       " ('t', -1529),\n",
       " ('d', -1534),\n",
       " ('says', -1536),\n",
       " ('needed', -1539),\n",
       " ('stop', -1552),\n",
       " ('watch', -1552),\n",
       " ('dog', -1552),\n",
       " ('thanks', -1572),\n",
       " ('y', -1585),\n",
       " ('inside', -1605),\n",
       " ('wants', -1619),\n",
       " ('job', -1624),\n",
       " ('relationship', -1631),\n",
       " ('b', -1632),\n",
       " ('realize', -1637),\n",
       " ('coffee', -1663),\n",
       " ('seriously', -1683),\n",
       " ('door', -1705),\n",
       " ('m', -1749),\n",
       " ('cold', -1808),\n",
       " ('fall', -1821),\n",
       " ('children', -1825),\n",
       " ('face', -1826),\n",
       " ('hours', -1827),\n",
       " ('decided', -1849),\n",
       " ('books', -1851),\n",
       " ('waiting', -1852),\n",
       " ('dream', -1863),\n",
       " ('knows', -1868),\n",
       " ('realized', -1874),\n",
       " ('awesome', -1885),\n",
       " ('pictures', -1887),\n",
       " ('women', -1895),\n",
       " ('lately', -1919),\n",
       " ('weeks', -1924),\n",
       " ('sunday', -1931),\n",
       " ('gave', -1946),\n",
       " ('brought', -1949),\n",
       " ('busy', -1961),\n",
       " ('tried', -2010),\n",
       " ('w', -2031),\n",
       " ('lots', -2043),\n",
       " ('walked', -2045),\n",
       " ('woke', -2054),\n",
       " ('feels', -2074),\n",
       " ('hear', -2081),\n",
       " ('bought', -2085),\n",
       " ('store', -2097),\n",
       " ('write', -2163),\n",
       " ('ask', -2208),\n",
       " ('help', -2232),\n",
       " ('sit', -2243),\n",
       " ('telling', -2258),\n",
       " ('water', -2289),\n",
       " ('supposed', -2293),\n",
       " ('hang', -2296),\n",
       " ('child', -2337),\n",
       " ('dance', -2346),\n",
       " ('wonder', -2357),\n",
       " ('anyways', -2369),\n",
       " ('ate', -2385),\n",
       " ('laugh', -2393),\n",
       " ('gone', -2397),\n",
       " ('sex', -2451),\n",
       " ('watched', -2484),\n",
       " ('perfect', -2500),\n",
       " ('den', -2500),\n",
       " ('wow', -2525),\n",
       " ('sitting', -2541),\n",
       " ('met', -2570),\n",
       " ('summer', -2640),\n",
       " ('sat', -2655),\n",
       " ('smile', -2671),\n",
       " ('bored', -2678),\n",
       " ('eating', -2679),\n",
       " ('guys', -2680),\n",
       " ('called', -2691),\n",
       " ('brother', -2704),\n",
       " ('ready', -2750),\n",
       " ('stay', -2766),\n",
       " ('hehe', -2788),\n",
       " ('walk', -2827),\n",
       " ('kept', -2843),\n",
       " ('boys', -2851),\n",
       " ('cause', -2875),\n",
       " ('body', -2879),\n",
       " ('mad', -2941),\n",
       " ('stupid', -2949),\n",
       " ('hey', -2956),\n",
       " ('sorry', -2975),\n",
       " ('saw', -2987),\n",
       " ('pain', -2990),\n",
       " ('mood', -3045),\n",
       " ('head', -3090),\n",
       " ('saturday', -3099),\n",
       " ('hope', -3100),\n",
       " ('hard', -3118),\n",
       " ('hot', -3125),\n",
       " ('glad', -3136),\n",
       " ('lunch', -3138),\n",
       " ('car', -3205),\n",
       " ('girls', -3208),\n",
       " ('took', -3287),\n",
       " ('sweet', -3380),\n",
       " ('thank', -3392),\n",
       " ('woman', -3433),\n",
       " ('friday', -3440),\n",
       " ('looked', -3460),\n",
       " ('remember', -3471),\n",
       " ('wonderful', -3478),\n",
       " ('talking', -3486),\n",
       " ('leave', -3498),\n",
       " ('o', -3514),\n",
       " ('ya', -3574),\n",
       " ('loved', -3623),\n",
       " ('parents', -3637),\n",
       " ('boy', -3648),\n",
       " ('makes', -3752),\n",
       " ('beautiful', -3768),\n",
       " ('asked', -3803),\n",
       " ('wait', -3809),\n",
       " ('getting', -3811),\n",
       " ('care', -3830),\n",
       " ('trying', -3862),\n",
       " ('food', -3940),\n",
       " ('knew', -3962),\n",
       " ('totally', -3983),\n",
       " ('favorite', -4142),\n",
       " ('phone', -4161),\n",
       " ('talked', -4183),\n",
       " ('tired', -4186),\n",
       " ('eyes', -4196),\n",
       " ('kids', -4198),\n",
       " ('crazy', -4209),\n",
       " ('anymore', -4246),\n",
       " ('school', -4251),\n",
       " ('hurt', -4262),\n",
       " ('weird', -4300),\n",
       " ('funny', -4314),\n",
       " ('sick', -4334),\n",
       " ('tonight', -4346),\n",
       " ('thinking', -4358),\n",
       " ('sleep', -4673),\n",
       " ('mother', -4691),\n",
       " ('sister', -4852),\n",
       " ('dinner', -4873),\n",
       " ('came', -4964),\n",
       " ('eat', -5017),\n",
       " ('room', -5052),\n",
       " ('having', -5273),\n",
       " ('tomorrow', -5282),\n",
       " ('person', -5299),\n",
       " ('excited', -5393),\n",
       " ('birthday', -5484),\n",
       " ('felt', -5557),\n",
       " ('sad', -5611),\n",
       " ('yesterday', -5658),\n",
       " ('wish', -5815),\n",
       " ('dad', -5904),\n",
       " ('yeah', -5910),\n",
       " ('nice', -5915),\n",
       " ('away', -5968),\n",
       " ('talk', -6012),\n",
       " ('weekend', -6143),\n",
       " ('family', -6227),\n",
       " ('mean', -6303),\n",
       " ('haha', -6482),\n",
       " ('heart', -6542),\n",
       " ('girl', -6551),\n",
       " ('guess', -6664),\n",
       " ('okay', -6674),\n",
       " ('u', -6807),\n",
       " ('bed', -6826),\n",
       " ('work', -6880),\n",
       " ('feeling', -6932),\n",
       " ('yes', -7131),\n",
       " ('wanted', -7280),\n",
       " ('morning', -7461),\n",
       " ('cuz', -7485),\n",
       " ('tell', -7631),\n",
       " ('hate', -7671),\n",
       " ('hair', -7820),\n",
       " ('baby', -7995),\n",
       " ('friend', -8244),\n",
       " ('miss', -8617),\n",
       " ('ok', -8642),\n",
       " ('house', -9162),\n",
       " ('friends', -10188),\n",
       " ('told', -11006),\n",
       " ('happy', -12010),\n",
       " ('lol', -14881),\n",
       " ('mom', -15218),\n",
       " ('love', -35837)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dict_boys_vs_girls.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stessa cosa per l'età!\n",
    "from collections import Counter\n",
    "\n",
    "# Lo dividiamo in 3 parti:\n",
    "# young vs rest\n",
    "# medium vs rest\n",
    "# old vs rest\n",
    "\n",
    "dict_young_vs_rest = dict()\n",
    "dict_medium_and_old = Counter(medium_dict) + Counter(old_dict)\n",
    "\n",
    "for word in young_words:\n",
    "    freq_y = young_dict[word]\n",
    "\n",
    "    try:\n",
    "        freq_r = dict_medium_and_old[word]\n",
    "    except KeyError:\n",
    "        freq_r = 0\n",
    "\n",
    "    dict_young_vs_rest[word] = freq_y - freq_r # se > 0 compare più nei giovani, se < 0 compare di più nelle altre classi\n",
    "\n",
    "########################\n",
    "\n",
    "dict_medium_vs_rest = dict()\n",
    "dict_young_and_old = Counter(young_dict) + Counter(old_dict)\n",
    "\n",
    "for word in medium_words:\n",
    "    freq_med = medium_dict[word]\n",
    "\n",
    "    try:\n",
    "        freq_re = dict_young_and_old[word]\n",
    "    except KeyError:\n",
    "        freq_re = 0\n",
    "\n",
    "    dict_medium_vs_rest[word] = freq_med - freq_re # se > 0 compare più nei medi, se < 0 compare di più nelle altre classi\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "dict_old_vs_rest = dict()\n",
    "dict_young_and_medium = Counter(young_dict) + Counter(medium_dict)\n",
    "\n",
    "for word in old_words:\n",
    "    freq_o = old_dict[word]\n",
    "\n",
    "    try:\n",
    "        freq_rest = dict_young_and_medium[word]\n",
    "    except KeyError:\n",
    "        freq_rest = 0\n",
    "\n",
    "    dict_old_vs_rest[word] = freq_o - freq_rest # se > 0 compare più nei vecchi, se < 0 compare di più nelle altre classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iraq', -1631),\n",
       " ('information', -3557),\n",
       " ('president', -3977),\n",
       " ('government', -4689),\n",
       " ('children', -5723),\n",
       " ('company', -6843),\n",
       " ('bush', -7615),\n",
       " ('books', -7741),\n",
       " ('com', -8182),\n",
       " ('power', -8185),\n",
       " ('church', -8279),\n",
       " ('state', -8395),\n",
       " ('war', -8512),\n",
       " ('american', -8563),\n",
       " ('women', -8705),\n",
       " ('john', -8786),\n",
       " ('woman', -9115),\n",
       " ('office', -9919),\n",
       " ('country', -9938),\n",
       " ('body', -10329),\n",
       " ('mother', -10779),\n",
       " ('men', -10825),\n",
       " ('case', -11347),\n",
       " ('line', -11745),\n",
       " ('number', -11839),\n",
       " ('living', -11934),\n",
       " ('site', -12047),\n",
       " ('city', -12454),\n",
       " ('white', -13117),\n",
       " ('months', -13552),\n",
       " ('water', -13555),\n",
       " ('small', -13592),\n",
       " ('group', -13817),\n",
       " ('news', -13963),\n",
       " ('kids', -14042),\n",
       " ('open', -14077),\n",
       " ('problem', -14132),\n",
       " ('set', -14383),\n",
       " ('black', -14710),\n",
       " ('early', -14720),\n",
       " ('looked', -15006),\n",
       " ('check', -15193),\n",
       " ('heard', -15477),\n",
       " ('taking', -15681),\n",
       " ('word', -15786),\n",
       " ('gave', -16066),\n",
       " ('weeks', -16198),\n",
       " ('hand', -16546),\n",
       " ('couple', -16746),\n",
       " ('true', -16771),\n",
       " ('words', -17125),\n",
       " ('knew', -17220),\n",
       " ('hear', -17229),\n",
       " ('ago', -17264),\n",
       " ('late', -17693),\n",
       " ('rest', -17748),\n",
       " ('story', -17828),\n",
       " ('run', -17830),\n",
       " ('gone', -18117),\n",
       " ('second', -18236),\n",
       " ('comes', -18318),\n",
       " ('asked', -18353),\n",
       " ('book', -18429),\n",
       " ('saying', -18469),\n",
       " ('hour', -18682),\n",
       " ('food', -18718),\n",
       " ('goes', -18753),\n",
       " ('minutes', -18955),\n",
       " ('bed', -19052),\n",
       " ('lost', -19083),\n",
       " ('interesting', -19331),\n",
       " ('seen', -19552),\n",
       " ('party', -19656),\n",
       " ('working', -19797),\n",
       " ('idea', -19813),\n",
       " ('phone', -19885),\n",
       " ('reading', -20098),\n",
       " ('eyes', -20284),\n",
       " ('free', -20328),\n",
       " ('change', -20694),\n",
       " ('high', -20745),\n",
       " ('making', -20864),\n",
       " ('face', -21140),\n",
       " ('heart', -21208),\n",
       " ('coming', -21668),\n",
       " ('able', -21949),\n",
       " ('family', -22169),\n",
       " ('past', -22216),\n",
       " ('e', -22515),\n",
       " ('felt', -22639),\n",
       " ('fact', -22674),\n",
       " ('half', -22727),\n",
       " ('different', -22765),\n",
       " ('reason', -22786),\n",
       " ('ask', -22848),\n",
       " ('far', -22994),\n",
       " ('hell', -23118),\n",
       " ('use', -23195),\n",
       " ('wrong', -23282),\n",
       " ('says', -23562),\n",
       " ('tonight', -23630),\n",
       " ('d', -23756),\n",
       " ('real', -23769),\n",
       " ('job', -24138),\n",
       " ('live', -24448),\n",
       " ('money', -24521),\n",
       " ('s', -24531),\n",
       " ('care', -24640),\n",
       " ('watch', -24772),\n",
       " ('soon', -24928),\n",
       " ('leave', -25024),\n",
       " ('stop', -25312),\n",
       " ('feeling', -25336),\n",
       " ('course', -25363),\n",
       " ('point', -25589),\n",
       " ('weekend', -25641),\n",
       " ('decided', -25775),\n",
       " ('write', -25921),\n",
       " ('believe', -26035),\n",
       " ('t', -26205),\n",
       " ('yesterday', -26490),\n",
       " ('room', -26570),\n",
       " ('help', -26616),\n",
       " ('thinking', -26762),\n",
       " ('music', -26781),\n",
       " ('car', -26831),\n",
       " ('remember', -27085),\n",
       " ('sleep', -27329),\n",
       " ('looking', -27626),\n",
       " ('hours', -27685),\n",
       " ('talking', -27872),\n",
       " ('makes', -27932),\n",
       " ('finally', -28076),\n",
       " ('game', -28541),\n",
       " ('head', -28984),\n",
       " ('play', -29078),\n",
       " ('found', -29291),\n",
       " ('mom', -29306),\n",
       " ('mind', -29488),\n",
       " ('years', -29649),\n",
       " ('saw', -29717),\n",
       " ('took', -30075),\n",
       " ('called', -30119),\n",
       " ('trying', -30408),\n",
       " ('hard', -30992),\n",
       " ('times', -31140),\n",
       " ('having', -31299),\n",
       " ('bit', -31498),\n",
       " ('start', -31825),\n",
       " ('girl', -31959),\n",
       " ('try', -32094),\n",
       " ('morning', -32297),\n",
       " ('started', -32331),\n",
       " ('wanted', -32332),\n",
       " ('tomorrow', -32512),\n",
       " ('later', -32727),\n",
       " ('probably', -33352),\n",
       " ('told', -33636),\n",
       " ('big', -33771),\n",
       " ('post', -34044),\n",
       " ('movie', -34415),\n",
       " ('old', -35535),\n",
       " ('read', -36678),\n",
       " ('cool', -36715),\n",
       " ('yes', -37051),\n",
       " ('place', -37409),\n",
       " ('hope', -38002),\n",
       " ('end', -38195),\n",
       " ('talk', -38216),\n",
       " ('left', -38224),\n",
       " ('friend', -38576),\n",
       " ('person', -38667),\n",
       " ('guy', -39160),\n",
       " ('house', -39266),\n",
       " ('ok', -39492),\n",
       " ('mean', -40347),\n",
       " ('happy', -40468),\n",
       " ('nice', -41673),\n",
       " ('getting', -42455),\n",
       " ('best', -42923),\n",
       " ('came', -42984),\n",
       " ('away', -43424),\n",
       " ('blog', -44748),\n",
       " ('tell', -44795),\n",
       " ('guess', -44854),\n",
       " ('stuff', -45002),\n",
       " ('pretty', -49205),\n",
       " ('friends', -56484),\n",
       " ('u', -56973),\n",
       " ('school', -63399),\n",
       " ('work', -77916),\n",
       " ('love', -104731)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dict_old_vs_rest.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "boys_best = sorted(dict_boys_vs_girls.items(), key=lambda x: x[1], reverse=True)[:150]\n",
    "girls_best = sorted(dict_boys_vs_girls.items(), key=lambda x: x[1], reverse=False)[:150]\n",
    "\n",
    "young_best = sorted(dict_young_vs_rest.items(), key=lambda x: x[1], reverse=True)[:150]\n",
    "medium_best = sorted(dict_medium_vs_rest.items(), key=lambda x: x[1], reverse=True)[:150]\n",
    "old_best = sorted(dict_old_vs_rest.items(), key=lambda x: x[1], reverse=True)[:150]\n",
    "\n",
    "# In particolare per i giovani, durante il preprocessing abbiamo trovato altre parole caratteristiche:\n",
    "\n",
    "other_young_words = [' bf ', ' gf ', 'bday', 'peeps', 'b/c', ' cos ',\n",
    "               ' jk ', 'wtf', 'WTF', 'omg', 'OMG', 'lol', 'omfg', 'OMFG', 'XD', 'wuts',\n",
    "                ' cuz ', ' ppl ', 'cant', 'nvr', ' enuf ', ' u ', ' n ', ' r ', ' luv ', 'haha', 'yeah', 'oh']\n",
    "\n",
    "parole_discriminanti = list(set([tuple[0] for tuple in boys_best + girls_best + young_best + other_young_words + medium_best + old_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parole_discriminanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora la parte più difficile. Per ognuna di queste parole, dobbiamo creare una variabile 0/1, che ci dice se quella parola è presente o meno nel testo\n",
    "\n",
    "def words_variables(post):\n",
    "    var_dict = {}\n",
    "    for parola in parole_discriminanti:\n",
    "        nome_variabile = f'has_{parola}'\n",
    "        var_dict[nome_variabile] = 1 if parola in post else 0\n",
    "    return pd.Series(var_dict)\n",
    "\n",
    "train_parole = train['post'].apply(words_variables)\n",
    "train = pd.concat([train, train_parole], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E a questo punto non resta altro se non creare la nuova variabile:\n",
    "\n",
    "# train['girl_word'] = train['post'].apply(lambda x: int(any(word in x for word in only_girls)))\n",
    "# train['boy_word'] = train['post'].apply(lambda x: int(any(word in x for word in only_boys)))\n",
    "\n",
    "# train['young_word'] = train['post'].apply(lambda x: int(any(word in x for word in only_young)))\n",
    "# train['medium_word'] = train['post'].apply(lambda x: int(any(word in x for word in only_medium)))\n",
    "# train['old_word'] = train['post'].apply(lambda x: int(any(word in x for word in only_old)))\n",
    "\n",
    "# # Factorize\n",
    "# train['girl_word'] = pd.factorize(train.girl_word)[0]\n",
    "# train['boy_word'] = pd.factorize(train.boy_word)[0]\n",
    "\n",
    "# train['young_word'] = pd.factorize(train.young_word)[0]\n",
    "# train['medium_word'] = pd.factorize(train.medium_word)[0]\n",
    "# train['old_word'] = pd.factorize(train.old_word)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>clean_post</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_class</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_emoticon</th>\n",
       "      <th>has_punctuation</th>\n",
       "      <th>has_toy</th>\n",
       "      <th>has_theloboy</th>\n",
       "      <th>has_match</th>\n",
       "      <th>...</th>\n",
       "      <th>has_start</th>\n",
       "      <th>has_pepper</th>\n",
       "      <th>has_speech</th>\n",
       "      <th>has_couple</th>\n",
       "      <th>has_kim</th>\n",
       "      <th>has_train</th>\n",
       "      <th>has_critical</th>\n",
       "      <th>has_shit</th>\n",
       "      <th>has_taskforce</th>\n",
       "      <th>has_remain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ooh shiny commenting</td>\n",
       "      <td>ooh shiny commenting</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wuts parade suked band battle kicked ass jims ...</td>\n",
       "      <td>wuts parade suked band battle kicked ass jims ...</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anymore concerned everyday bold faced liar ahe...</td>\n",
       "      <td>anymore concerned everyday bold faced liar ahe...</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roof sunset posted paul</td>\n",
       "      <td>roof sunset posted paul</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gawd luv nanny absolutely greatest woman earth...</td>\n",
       "      <td>gawd luv nanny absolutely greatest woman earth...</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>thursday jun p m forgotten conversations lunch...</td>\n",
       "      <td>thursday jun p m forgotten conversations lunch...</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>cause grief trying hard told far kept trying f...</td>\n",
       "      <td>cause grief trying hard told far kept trying f...</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>talking hear whispering ultimate hubris believ...</td>\n",
       "      <td>talking hear whispering ultimate hubris believ...</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>yeah heard phantom planet alright heard song c...</td>\n",
       "      <td>yeah heard phantom planet alright heard song c...</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>john carmela visit</td>\n",
       "      <td>john carmela visit</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 653 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   post  \\\n",
       "0                                  ooh shiny commenting   \n",
       "1     wuts parade suked band battle kicked ass jims ...   \n",
       "2     anymore concerned everyday bold faced liar ahe...   \n",
       "3                               roof sunset posted paul   \n",
       "4     gawd luv nanny absolutely greatest woman earth...   \n",
       "...                                                 ...   \n",
       "1034  thursday jun p m forgotten conversations lunch...   \n",
       "1035  cause grief trying hard told far kept trying f...   \n",
       "1036  talking hear whispering ultimate hubris believ...   \n",
       "1037  yeah heard phantom planet alright heard song c...   \n",
       "1038                                 john carmela visit   \n",
       "\n",
       "                                             clean_post  gender  age_class  \\\n",
       "0                                  ooh shiny commenting  female          0   \n",
       "1     wuts parade suked band battle kicked ass jims ...    male          0   \n",
       "2     anymore concerned everyday bold faced liar ahe...  female          1   \n",
       "3                               roof sunset posted paul    male          1   \n",
       "4     gawd luv nanny absolutely greatest woman earth...  female          1   \n",
       "...                                                 ...     ...        ...   \n",
       "1034  thursday jun p m forgotten conversations lunch...  female          1   \n",
       "1035  cause grief trying hard told far kept trying f...  female          0   \n",
       "1036  talking hear whispering ultimate hubris believ...  female          2   \n",
       "1037  yeah heard phantom planet alright heard song c...    male          1   \n",
       "1038                                 john carmela visit    male          2   \n",
       "\n",
       "      word_count  has_emoticon  has_punctuation  has_toy  has_theloboy  \\\n",
       "0              3             0                0        0             0   \n",
       "1             15             0                0        0             0   \n",
       "2             25             0                0        0             0   \n",
       "3              4             0                0        0             0   \n",
       "4            204             0                0        0             0   \n",
       "...          ...           ...              ...      ...           ...   \n",
       "1034          30             0                0        0             0   \n",
       "1035          63             0                0        0             0   \n",
       "1036          30             0                0        0             0   \n",
       "1037          32             0                0        0             0   \n",
       "1038           3             0                0        0             0   \n",
       "\n",
       "      has_match  ...  has_start  has_pepper  has_speech  has_couple  has_kim  \\\n",
       "0             0  ...          0           0           0           0        0   \n",
       "1             0  ...          0           0           0           0        0   \n",
       "2             0  ...          0           0           0           0        0   \n",
       "3             0  ...          0           0           0           0        0   \n",
       "4             0  ...          0           0           0           0        0   \n",
       "...         ...  ...        ...         ...         ...         ...      ...   \n",
       "1034          0  ...          0           0           0           0        0   \n",
       "1035          0  ...          0           0           0           0        0   \n",
       "1036          0  ...          0           0           0           0        0   \n",
       "1037          0  ...          0           0           0           0        0   \n",
       "1038          0  ...          0           0           0           0        0   \n",
       "\n",
       "      has_train  has_critical  has_shit  has_taskforce  has_remain  \n",
       "0             0             0         0              0           0  \n",
       "1             0             0         0              0           0  \n",
       "2             0             0         1              0           0  \n",
       "3             0             0         0              0           0  \n",
       "4             0             0         0              0           0  \n",
       "...         ...           ...       ...            ...         ...  \n",
       "1034          0             0         0              0           0  \n",
       "1035          0             0         0              0           0  \n",
       "1036          0             0         0              0           0  \n",
       "1037          0             0         0              0           0  \n",
       "1038          0             0         0              0           0  \n",
       "\n",
       "[1000 rows x 653 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Passiamo ora a vedere la presenza di URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: age_class, dtype: int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.post.str.contains('urlLink')].age_class.value_counts()\n",
    "\n",
    "# sembra che i giovanissimi ne usino di meno..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['has_URL'] = train['post'].str.contains('urlLink').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunghezza del post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_class\n",
       "0    66.029674\n",
       "1    78.481799\n",
       "2    87.673469\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('age_class')['word_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence count, average word length e average sentence length (NON DISCRIMINANO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentence counts\n",
    "# train[\"sent_count\"] = train[\"post\"].map(lambda x: len(sent_tokenize(x)))\n",
    "# # Average word length\n",
    "# train[\"avg_word_len\"] = train[\"post\"].map(lambda x: np.mean([len(w) for w in str(x).split()])).fillna(0)\n",
    "# # Average sentence length\n",
    "# train[\"avg_sent_len\"] = train[\"post\"].map(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)])).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.groupby('age_class')['sent_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP', 'VBD', 'VBG', 'IN', 'DT', 'NN', 'WRB', 'DT', 'VBD']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = []\n",
    "\n",
    "sentence = \"I was running down the street when this happened\"\n",
    "for token in nlp(sentence):\n",
    "    pos_list.append(token.tag_)\n",
    "\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PRP': 1, 'VBD': 2, 'VBG': 1, 'IN': 1, 'DT': 2, 'NN': 1, 'WRB': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_counter(text):\n",
    "    pos_list = []\n",
    "    for token in nlp(text):\n",
    "        pos_list.append(token.tag_)\n",
    "\n",
    "    return dict(Counter(pos_list))\n",
    "\n",
    "def pos_variables(text):\n",
    "    \n",
    "    verbs = pos_counter(text).get('VBD', 0) + pos_counter(text).get('VBG', 0) + pos_counter(text).get('VB', 0) + pos_counter(text).get('VBP', 0) + pos_counter(text).get('VBN', 0) + pos_counter(text).get('VBZ', 0)\n",
    "    prepositions = pos_counter(text).get('PRP', 0)\n",
    "    articles = pos_counter(text).get('DT', 0)\n",
    "\n",
    "    return verbs, prepositions, articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/simonefacchiano/Desktop/Data Science/SL/Project/altre_feature.ipynb Cella 53\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(pos_variables)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train[[\u001b[39m'\u001b[39m\u001b[39mverbs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprepositions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mextract(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m((\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+), (\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+), (\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train[[\u001b[39m'\u001b[39m\u001b[39mverbs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprepositions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m train[[\u001b[39m'\u001b[39m\u001b[39mverbs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprepositions\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1144\u001b[0m             values,\n\u001b[1;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1147\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/simonefacchiano/Desktop/Data Science/SL/Project/altre_feature.ipynb Cella 53\u001b[0m in \u001b[0;36mpos_variables\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_variables\u001b[39m(text):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     verbs \u001b[39m=\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mVBD\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mVBG\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mVB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mVBP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mVBN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mVBZ\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     prepositions \u001b[39m=\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mPRP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     articles \u001b[39m=\u001b[39m pos_counter(text)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mDT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;32m/Users/simonefacchiano/Desktop/Data Science/SL/Project/altre_feature.ipynb Cella 53\u001b[0m in \u001b[0;36mpos_counter\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_counter\u001b[39m(text):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     pos_list \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(text):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         pos_list\u001b[39m.\u001b[39mappend(token\u001b[39m.\u001b[39mtag_)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonefacchiano/Desktop/Data%20Science/SL/Project/altre_feature.ipynb#Y130sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(Counter(pos_list))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    126\u001b[0m batch_id \u001b[39m=\u001b[39m Tok2VecListener\u001b[39m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m listener \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/with_array.py:40\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_forward(cast(Model[List2d, List2d], model), Xseq, is_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/with_array.py:75\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     73\u001b[0m lengths \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39masarray1i([\u001b[39mlen\u001b[39m(seq) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m Xs])\n\u001b[1;32m     74\u001b[0m Xf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(Xs, pad\u001b[39m=\u001b[39mpad)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m Yf, get_dXf \u001b[39m=\u001b[39m layer(Xf, is_train)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYs: List2d) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List2d:\n\u001b[1;32m     78\u001b[0m     dYf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(dYs, pad\u001b[39m=\u001b[39mpad)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/residual.py:40\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m d_output \u001b[39m+\u001b[39m dX\n\u001b[0;32m---> 40\u001b[0m Y, backprop_layer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m](X, is_train)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m [X[i] \u001b[39m+\u001b[39m Y[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/expand_window.py:23\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m _expand_window_ragged(model, X)\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m _expand_window_floats(model, X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/layers/expand_window.py:31\u001b[0m, in \u001b[0;36m_expand_window_floats\u001b[0;34m(model, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m nW \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mwindow_size\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(X) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mseq2col(X, nW)\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(X) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train['pos'] = train['post'].apply(pos_variables)\n",
    "\n",
    "train[['verbs', 'prepositions', 'articles']] = train['pos'].str.extract(r'\\((\\d+), (\\d+), (\\d+)\\)')\n",
    "train[['verbs', 'prepositions', 'articles']] = train[['verbs', 'prepositions', 'articles']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['gender'] = train['gender'].map({'male': 1, 'female': 0})\n",
    "\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(train, 'train_features_paper.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['post', 'clean_post', 'gender', 'age_class', 'word_count',\n",
       "       'has_emoticon', 'has_punctuation', 'has_believe', 'has_version',\n",
       "       'has_com', 'has_woke', 'has_lunch', 'has_spent', 'has_power',\n",
       "       'has_friends', 'has_ago', 'has_stupid', 'has_watch', 'has_living',\n",
       "       'has_found', 'has_cuz', 'has_trying', 'has_pay', 'has_im',\n",
       "       'has_ah', 'has_ya', 'has_seen', 'has_college', 'has_americans',\n",
       "       'has_interesting', 'has_plus', 'has_michael', 'has_c', 'has_story',\n",
       "       'has_beer', 'has_camp', 'has_O', 'has_room', 'has_online',\n",
       "       'has_john', 'has_site', 'has_kept', 'has_place', 'has_de',\n",
       "       'has_blue', 'has_human', 'has_face', 'has_finally', 'has_told',\n",
       "       'has_mad', 'has_hopefully', 'has_hand', 'has_morning', 'has_bed',\n",
       "       'has_play', 'has_small', 'has_care', 'has_drinking', 'has_america',\n",
       "       'has_works', 'has_early', 'has_dad', 'has_beautiful', 'has_school',\n",
       "       'has_soon', 'has_music', 'has_crap', 'has_ha', 'has_funny',\n",
       "       'has_use', 'has_won', 'has_brother', 'has_tell', 'has_mail',\n",
       "       'has_haiz', 'has_large', 'has_hate', 'has_weekend', 'has_lost',\n",
       "       'has_saw', 'has_street', 'has_wow', 'has_wedding', 'has_hair',\n",
       "       'has_reality', 'has_problem', 'has_hot', 'has_haha', 'has_band',\n",
       "       'has_word', 'has_wrote', 'has_wish', 'has_deal', 'has_called',\n",
       "       'has_thoughts', 'has_sort', 'has_black', 'has_tomorrow', 'has_d',\n",
       "       'has_paid', 'has_order', 'has_rest', 'has_girl', 'has_check',\n",
       "       'has_moving', 'has_sweet', 'has_thank', 'has_class', 'has_write',\n",
       "       'has_recently', 'has_yeah', 'has_search', 'has_peace',\n",
       "       'has_project', 'has_feels', 'has_enjoy', 'has_past', 'has_idea',\n",
       "       'has_family', 'has_group', 'has_type', 'has_phone', 'has_ate',\n",
       "       'has_taking', 'has_series', 'has_reason', 'has_store', 'has_u',\n",
       "       'has_sister', 'has_lack', 'has_current', 'has_information',\n",
       "       'has_future', 'has_headed', 'has_sleep', 'has_smile', 'has_kids',\n",
       "       'has_tonight', 'has_security', 'has_fucking', 'has_bored',\n",
       "       'has_shows', 'has_asked', 'has_x', 'has_president', 'has_shall',\n",
       "       'has_loved', 'has_plans', 'has_church', 'has_case', 'has_dun',\n",
       "       'has_walk', 'has_boys', 'has_spend', 'has_mi', 'has_places',\n",
       "       'has_title', 'has_y', 'has_hours', 'has_excited', 'has_summer',\n",
       "       'has_death', 'has_service', 'has_real', 'has_star', 'has_child',\n",
       "       'has_points', 'has_click', 'has_person', 'has_public', 'has_start',\n",
       "       'has_goes', 'has_internet', 'has_mr', 'has_gave', 'has_money',\n",
       "       'has_given', 'has_strange', 'has_process', 'has_telling',\n",
       "       'has_imagine', 'has_lead', 'has_far', 'has_month', 'has_white',\n",
       "       'has_fact', 'has_alot', 'has_probably', 'has_thinking', 'has_hard',\n",
       "       'has_report', 'has_open', 'has_film', 'has_starting',\n",
       "       'has_finished', 'has_W', 'has_felt', 'has_building', 'has_leave',\n",
       "       'has_student', 'has_according', 'has_walked', 'has_boss',\n",
       "       'has_wrong', 'has_big', 'has_cd', 'has_couple', 'has_party',\n",
       "       'has_played', 'has_laugh', 'has_link', 'has_da', 'has_den',\n",
       "       'has_women', 'has_sad', 'has_cry', 'has_decided', 'has_perfect',\n",
       "       'has_eat', 'has_house', 'has_computer', 'has_parents', 'has_yea',\n",
       "       'has_states', 'has_interested', 'has_bye', 'has_totally',\n",
       "       'has_american', 'has_n', 'has_r', 'has_knew', 'has_talk',\n",
       "       'has_shot', 'has_le', 'has_woman', 'has_issues', 'has_bill',\n",
       "       'has_likely', 'has_live', 'has_busy', 'has_pissed', 'has_e',\n",
       "       'has_crazy', 'has_feeling', 'has_stories', 'has_gay',\n",
       "       'has_sitting', 'has_video', 'has_entire', 'has_town', 'has_share',\n",
       "       'has_p', 'has_managed', 'has_line', 'has_wife', 'has_words',\n",
       "       'has_ideas', 'has_weird', 'has_anymore', 'has_situation', 'has_X',\n",
       "       'has_team', 'has_body', 'has_based', 'has_took', 'has_record',\n",
       "       'has_cute', 'has_country', 'has_experience', 'has_drink',\n",
       "       'has_heart', 'has_rock', 'has_guess', 'has_eyes', 'has_important',\n",
       "       'has_cos', 'has_forever', 'has_truly', 'has_test', 'has_figure',\n",
       "       'has_movies', 'has_boy', 'has_g', 'has_state', 'has_students',\n",
       "       'has_heard', 'has_simply', 'has_till', 'has_system', 'has_page',\n",
       "       'has_create', 'has_mean', 'has_cool', 'has_al', 'has_hope',\n",
       "       'has_paul', 'has_coffee', 'has_cause', 'has_true', 'has_web',\n",
       "       'has_york', 'has_says', 'has_form', 'has_hurt', 'has_national',\n",
       "       'has_book', 'has_amazing', 'has_apparently', 'has_ask',\n",
       "       'has_boring', 'has_reading', 'has_looking', 'has_s', 'has_quick',\n",
       "       'has_looked', 'has_game', 'has_history', 'has_united',\n",
       "       'has_community', 'has_action', 'has_forget', 'has_glad', 'has_l',\n",
       "       'has_fuck', 'has_break', 'has_doesnt', 'has_supposed', 'has_hear',\n",
       "       'has_bit', 'has_hmm', 'has_ill', 'has_working', 'has_h',\n",
       "       'has_girls', 'has_car', 'has_random', 'has_late', 'has_main',\n",
       "       'has_comes', 'has_second', 'has_saying', 'has_general', 'has_stay',\n",
       "       'has_job', 'has_realize', 'has_songs', 'has_sat', 'has_forward',\n",
       "       'has_sit', 'has_ur', 'has_train', 'has_road', 'has_kid', 'has_o',\n",
       "       'has_yes', 'has_website', 'has_remember', 'has_times', 'has_email',\n",
       "       'has_wonder', 'has_water', 'has_games', 'has_tired', 'has_dont',\n",
       "       'has_alright', 'has_hehe', 'has_course', 'has_bought',\n",
       "       'has_watched', 'has_homework', 'has_sorry', 'has_seriously',\n",
       "       'has_study', 'has_baby', 'has_hang', 'has_period', 'has_million',\n",
       "       'has_talked', 'has_afternoon', 'has_question', 'has_sigh',\n",
       "       'has_end', 'has_local', 'has_vote', 'has_listening', 'has_posts',\n",
       "       'has_business', 'has_completely', 'has_news', 'has_minute',\n",
       "       'has_having', 'has_moved', 'has_area', 'has_issue', 'has_b',\n",
       "       'has_miss', 'has_wonderful', 'has_visit', 'has_number',\n",
       "       'has_jumper', 'has_la', 'has_including', 'has_coz', 'has_coming',\n",
       "       'has_anyways', 'has_possible', 'has_society', 'has_different',\n",
       "       'has_mom', 'has_dinner', 'has_media', 'has_military', 'has_style',\n",
       "       'has_government', 'has_came', 'has_mood', 'has_kerry',\n",
       "       'has_company', 'has_w', 'has_getting', 'has_movie', 'has_seeing',\n",
       "       'has_eating', 'has_t', 'has_post', 'has_george', 'has_win',\n",
       "       'has_song', 'has_wake', 'has_listen', 'has_mother', 'has_ready',\n",
       "       'has_members', 'has_hour', 'has_following', 'has_change',\n",
       "       'has_guys', 'has_especially', 'has_months', 'has_note',\n",
       "       'has_sense', 'has_mind', 'has_city', 'has_years', 'has_away',\n",
       "       'has_office', 'has_example', 'has_birthday', 'has_final', 'has_pm',\n",
       "       'has_major', 'has_bar', 'has_walking', 'has_apartment',\n",
       "       'has_yesterday', 'has_favorite', 'has_despite', 'has_hey',\n",
       "       'has_men', 'has_wait', 'has_hoping', 'has_saturday', 'has_simple',\n",
       "       'has_ok', 'has_friday', 'has_able', 'has_sucks', 'has_evening',\n",
       "       'has_article', 'has_tt', 'has_awesome', 'has_program', 'has_blah',\n",
       "       'has_list', 'has_stop', 'has_happy', 'has_conversation',\n",
       "       'has_view', 'has_political', 'has_sex', 'has_relationship',\n",
       "       'has_ ', 'has_set', 'has_ended', 'has_instead', 'has_kill',\n",
       "       'has_thats', 'has_die', 'has_bus', 'has_minutes', 'has_force',\n",
       "       'has_club', 'has_yay', 'has_talking', 'has_married', 'has_high',\n",
       "       'has_friend', 'has_lol', 'has_bush', 'has_free', 'has_books',\n",
       "       'has_making', 'has_hell', 'has_weeks', 'has_iraq', 'has_damn',\n",
       "       'has_definitely', 'has_blog', 'has_point', 'has_dance', 'has_wat',\n",
       "       'has_half', 'has_pain', 'has_help', 'has_fast', 'has_support',\n",
       "       'has_k', 'has_english', 'has_makes', 'has_season', 'has_album',\n",
       "       'has_space', 'has_law', 'has_children', 'has_trip', 'has_met',\n",
       "       'has_love', 'has_nice', 'has_wanted', 'has_card', 'has_war',\n",
       "       'has_playing', 'has_etc', 'has_worth', 'has_head', 'has_weather',\n",
       "       'has_food', 'has_basically', 'has_okay', 'has_run', 'has_drunk',\n",
       "       'has_sick', 'has_level', 'has_work', 'has_gone', 'has_teacher',\n",
       "       'has_URL', 'has_capital'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "#### We do all the previous steps again, this time on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('/Users/simonefacchiano/Desktop/Data Science/SL/Project/SL-Final-Project/test.json')\n",
    "test_clean = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/train_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.insert(1, 'clean_post', test_clean.post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['age_class'] = pd.cut(\n",
    "        test[\"age\"],\n",
    "        bins=[12, 18, 28, 50],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(\"int\")\n",
    "\n",
    "test = test.drop('age', axis = 1)\n",
    "\n",
    "\n",
    "test_clean['age_class'] = pd.cut(\n",
    "        test_clean[\"age\"],\n",
    "        bins=[12, 18, 28, 50],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(\"int\")\n",
    "\n",
    "test_clean = test_clean.drop('age', axis = 1)\n",
    "\n",
    "test['word_count'] = test['post'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = [r':\\)', r':-\\)', r':\\(', r':-\\(', r';\\)', r';-\\)', r':D', r':-D', r':P', r':-P', r':O', r':-O', r':\\|', r':-\\|', r'>:\\(', r\":'\\(\", r\":'-\\(\", r'XD', r'<3', r':3', r'>:-0']\n",
    "\n",
    "test['has_emoticon'] = test['post'].str.contains('|'.join(emoticons), regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "characters = ['...', '!!', '??', '?!']\n",
    "pattern = '|'.join([re.escape(char) for char in characters])\n",
    "\n",
    "test['has_punctuation'] = test['post'].str.contains(pattern).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary & discriminative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hope',\n",
       " 'hehe',\n",
       " 'weeks',\n",
       " 'york',\n",
       " 'sister',\n",
       " 'friend',\n",
       " 'members',\n",
       " 'anymore',\n",
       " 'die',\n",
       " 'click',\n",
       " 'doesnt',\n",
       " 'totally',\n",
       " 'homework',\n",
       " 'plans',\n",
       " 'past',\n",
       " 'asked',\n",
       " 'sort',\n",
       " 'cd',\n",
       " 'game',\n",
       " 'lol',\n",
       " 'military',\n",
       " 'fast',\n",
       " 'r',\n",
       " 'afternoon',\n",
       " 'boy',\n",
       " 'w',\n",
       " 'places',\n",
       " 'sad',\n",
       " 'thinking',\n",
       " 'completely',\n",
       " 'thank',\n",
       " 'ha',\n",
       " 'group',\n",
       " 'support',\n",
       " 'pissed',\n",
       " 'comes',\n",
       " 'knew',\n",
       " 'seriously',\n",
       " 'weird',\n",
       " 'list',\n",
       " 'drinking',\n",
       " 'love',\n",
       " 'main',\n",
       " 'type',\n",
       " 'shall',\n",
       " 'enjoy',\n",
       " 'shows',\n",
       " 'sex',\n",
       " 'friends',\n",
       " 'americans',\n",
       " 'place',\n",
       " 'wonder',\n",
       " 'force',\n",
       " 'care',\n",
       " 'wat',\n",
       " 'ill',\n",
       " 'fact',\n",
       " 'future',\n",
       " 'yeah',\n",
       " 'makes',\n",
       " 'alot',\n",
       " 'bed',\n",
       " 'soon',\n",
       " 'thats',\n",
       " 'sorry',\n",
       " 'deal',\n",
       " 'company',\n",
       " 'public',\n",
       " 'excited',\n",
       " 'trying',\n",
       " 'moving',\n",
       " 'question',\n",
       " 'finally',\n",
       " 'supposed',\n",
       " 'paul',\n",
       " 'pay',\n",
       " 'summer',\n",
       " 'listen',\n",
       " 'record',\n",
       " 'problem',\n",
       " 'bye',\n",
       " 'weekend',\n",
       " 'sit',\n",
       " 'club',\n",
       " 'cool',\n",
       " 'rock',\n",
       " 'entire',\n",
       " 'state',\n",
       " 'birthday',\n",
       " 'title',\n",
       " 'money',\n",
       " 'mad',\n",
       " 'saying',\n",
       " 'yay',\n",
       " 'mood',\n",
       " 'teacher',\n",
       " 'mom',\n",
       " 'truly',\n",
       " 'second',\n",
       " 'imagine',\n",
       " 'times',\n",
       " 'probably',\n",
       " 'favorite',\n",
       " 'break',\n",
       " 'cause',\n",
       " 'big',\n",
       " 'word',\n",
       " 'ur',\n",
       " 'white',\n",
       " 'history',\n",
       " 'society',\n",
       " 'possible',\n",
       " 'head',\n",
       " 'leave',\n",
       " 'current',\n",
       " 'getting',\n",
       " 'star',\n",
       " 'experience',\n",
       " 'blue',\n",
       " 'jumper',\n",
       " 'beautiful',\n",
       " 'dinner',\n",
       " 'saw',\n",
       " 'movies',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'la',\n",
       " 'wonderful',\n",
       " 'alright',\n",
       " 'camp',\n",
       " 'forever',\n",
       " 'basically',\n",
       " 'sleep',\n",
       " 'visit',\n",
       " 'service',\n",
       " 'im',\n",
       " 'bored',\n",
       " 'o',\n",
       " 'n',\n",
       " 'amazing',\n",
       " 'despite',\n",
       " 'season',\n",
       " 'general',\n",
       " 'tired',\n",
       " 'trip',\n",
       " 'reading',\n",
       " 'songs',\n",
       " 'glad',\n",
       " 'far',\n",
       " 'conversation',\n",
       " 'watch',\n",
       " 'yesterday',\n",
       " 'half',\n",
       " 'girl',\n",
       " 'haha',\n",
       " 'open',\n",
       " 'coffee',\n",
       " 'friday',\n",
       " 'local',\n",
       " 'bought',\n",
       " 'talk',\n",
       " 'article',\n",
       " 'living',\n",
       " 'al',\n",
       " 'final',\n",
       " 'spent',\n",
       " 'states',\n",
       " 'ask',\n",
       " 'e',\n",
       " 'walked',\n",
       " 'sitting',\n",
       " 'dance',\n",
       " 'happy',\n",
       " 'community',\n",
       " 'dad',\n",
       " 'women',\n",
       " 'played',\n",
       " 'lack',\n",
       " 'hot',\n",
       " 'den',\n",
       " 'forward',\n",
       " 'black',\n",
       " 'sweet',\n",
       " 'playing',\n",
       " 'yes',\n",
       " 'sense',\n",
       " 'boring',\n",
       " 'worth',\n",
       " 'O',\n",
       " 'blog',\n",
       " 'wait',\n",
       " 'dont',\n",
       " 'stupid',\n",
       " 'project',\n",
       " 'found',\n",
       " 'large',\n",
       " 'based',\n",
       " 'ended',\n",
       " 'tomorrow',\n",
       " 'forget',\n",
       " 'sucks',\n",
       " 'style',\n",
       " 'minute',\n",
       " 'ok',\n",
       " 'free',\n",
       " 'wake',\n",
       " 'working',\n",
       " 'million',\n",
       " 'lunch',\n",
       " 'coming',\n",
       " 'team',\n",
       " 'win',\n",
       " 'wedding',\n",
       " 'program',\n",
       " 'sick',\n",
       " 'called',\n",
       " 'high',\n",
       " 'miss',\n",
       " 'month',\n",
       " 'cuz',\n",
       " 'study',\n",
       " 'work',\n",
       " 'k',\n",
       " 'paid',\n",
       " 'hear',\n",
       " 'course',\n",
       " 'album',\n",
       " 'ideas',\n",
       " 'kill',\n",
       " 'stay',\n",
       " 'hell',\n",
       " 'mind',\n",
       " 'awesome',\n",
       " 's',\n",
       " 'com',\n",
       " 'film',\n",
       " 'including',\n",
       " 'strange',\n",
       " 'city',\n",
       " 'p',\n",
       " 'figure',\n",
       " 'line',\n",
       " 'finished',\n",
       " 'words',\n",
       " 'bus',\n",
       " 'face',\n",
       " 'peace',\n",
       " 'okay',\n",
       " 'series',\n",
       " 'wrong',\n",
       " 'fuck',\n",
       " 'seen',\n",
       " 'early',\n",
       " 'real',\n",
       " 'band',\n",
       " 'especially',\n",
       " 'child',\n",
       " 'coz',\n",
       " 'play',\n",
       " 'W',\n",
       " 'students',\n",
       " 'george',\n",
       " 'vote',\n",
       " 'bush',\n",
       " 'start',\n",
       " 'issue',\n",
       " 'hurt',\n",
       " 'feels',\n",
       " 'news',\n",
       " 'de',\n",
       " 'card',\n",
       " 'gone',\n",
       " 'games',\n",
       " 'damn',\n",
       " 'y',\n",
       " 'church',\n",
       " 'hate',\n",
       " 'version',\n",
       " 'view',\n",
       " 'recently',\n",
       " 'power',\n",
       " 'ya',\n",
       " 'hand',\n",
       " 'eating',\n",
       " 'family',\n",
       " 'michael',\n",
       " 'issues',\n",
       " 'eat',\n",
       " 'music',\n",
       " 'reality',\n",
       " 'having',\n",
       " 'english',\n",
       " 'idea',\n",
       " 'heart',\n",
       " 'kid',\n",
       " 'student',\n",
       " 'gave',\n",
       " 'america',\n",
       " 'looked',\n",
       " 'bit',\n",
       " 'loved',\n",
       " 'plus',\n",
       " 'wow',\n",
       " 'baby',\n",
       " 'run',\n",
       " 'movie',\n",
       " 'building',\n",
       " 'pain',\n",
       " 'true',\n",
       " 'goes',\n",
       " 'crazy',\n",
       " 'lost',\n",
       " 'process',\n",
       " 'point',\n",
       " 'moved',\n",
       " 'starting',\n",
       " 'thoughts',\n",
       " 'interested',\n",
       " 'according',\n",
       " 'likely',\n",
       " 'years',\n",
       " 'lead',\n",
       " 'mean',\n",
       " 'eyes',\n",
       " 'simple',\n",
       " 'train',\n",
       " 'street',\n",
       " 'woke',\n",
       " 'u',\n",
       " 'kerry',\n",
       " 'ate',\n",
       " 'g',\n",
       " 'da',\n",
       " 'kept',\n",
       " 'decided',\n",
       " 'body',\n",
       " 'president',\n",
       " 'rest',\n",
       " 'web',\n",
       " 'following',\n",
       " 'human',\n",
       " 'website',\n",
       " 'security',\n",
       " 'smile',\n",
       " 'sigh',\n",
       " 'site',\n",
       " 'create',\n",
       " 'guess',\n",
       " 'cry',\n",
       " 'wrote',\n",
       " 'school',\n",
       " 'h',\n",
       " 'business',\n",
       " 'john',\n",
       " 'interesting',\n",
       " 'told',\n",
       " 'took',\n",
       " 'mr',\n",
       " 'gay',\n",
       " 'definitely',\n",
       " 'won',\n",
       " 'song',\n",
       " 'kids',\n",
       " 'etc',\n",
       " 'house',\n",
       " 'making',\n",
       " 'sat',\n",
       " 'small',\n",
       " 'post',\n",
       " 'pm',\n",
       " 'l',\n",
       " 'evening',\n",
       " 'instead',\n",
       " 'government',\n",
       " 'mi',\n",
       " 'managed',\n",
       " 'hour',\n",
       " 'guys',\n",
       " 'yea',\n",
       " 'apparently',\n",
       " 'national',\n",
       " 'story',\n",
       " 'car',\n",
       " 'spend',\n",
       " 'wish',\n",
       " 'weather',\n",
       " 'level',\n",
       " 'food',\n",
       " 'quick',\n",
       " 'b',\n",
       " 'books',\n",
       " 'random',\n",
       " 'death',\n",
       " 'blah',\n",
       " 'college',\n",
       " 'help',\n",
       " 'woman',\n",
       " 'search',\n",
       " 'space',\n",
       " 'war',\n",
       " 'mail',\n",
       " 'online',\n",
       " 'test',\n",
       " 'married',\n",
       " 'phone',\n",
       " 'X',\n",
       " 'listening',\n",
       " 'period',\n",
       " 'wanted',\n",
       " 'tonight',\n",
       " 'wife',\n",
       " 'headed',\n",
       " 'video',\n",
       " 'd',\n",
       " 'perfect',\n",
       " 'heard',\n",
       " 'email',\n",
       " 'link',\n",
       " 'watched',\n",
       " 'stop',\n",
       " 'simply',\n",
       " 'given',\n",
       " 'check',\n",
       " 'beer',\n",
       " 'late',\n",
       " 'case',\n",
       " 'fucking',\n",
       " 'came',\n",
       " 'class',\n",
       " 'person',\n",
       " 'computer',\n",
       " 'internet',\n",
       " 'number',\n",
       " 'felt',\n",
       " 'major',\n",
       " 'end',\n",
       " 'realize',\n",
       " 'nice',\n",
       " 'room',\n",
       " 'mother',\n",
       " 'girls',\n",
       " ' ',\n",
       " 'store',\n",
       " 'months',\n",
       " 'x',\n",
       " 'hmm',\n",
       " 'live',\n",
       " 'system',\n",
       " 'cute',\n",
       " 'feeling',\n",
       " 'morning',\n",
       " 'stories',\n",
       " 'american',\n",
       " 'reason',\n",
       " 'area',\n",
       " 'couple',\n",
       " 'works',\n",
       " 'write',\n",
       " 'dun',\n",
       " 'haiz',\n",
       " 'looking',\n",
       " 'till',\n",
       " 'says',\n",
       " 'apartment',\n",
       " 'ah',\n",
       " 'example',\n",
       " 'children',\n",
       " 'country',\n",
       " 'town',\n",
       " 'change',\n",
       " 'page',\n",
       " 'posts',\n",
       " 'anyways',\n",
       " 'form',\n",
       " 'busy',\n",
       " 'report',\n",
       " 'hoping',\n",
       " 'crap',\n",
       " 'united',\n",
       " 'met',\n",
       " 'hopefully',\n",
       " 'believe',\n",
       " 'away',\n",
       " 'different',\n",
       " 'talking',\n",
       " 'cos',\n",
       " 'ready',\n",
       " 'water',\n",
       " 'law',\n",
       " 'ago',\n",
       " 'situation',\n",
       " 'important',\n",
       " 'minutes',\n",
       " 'boys',\n",
       " 'hang',\n",
       " 'political',\n",
       " 'media',\n",
       " 'funny',\n",
       " 'drunk',\n",
       " 'use',\n",
       " 'drink',\n",
       " 'information',\n",
       " 'hard',\n",
       " 'set',\n",
       " 'hair',\n",
       " 'boss',\n",
       " 'laugh',\n",
       " 'able',\n",
       " 'points',\n",
       " 'relationship',\n",
       " 'office',\n",
       " 'bar',\n",
       " 'action',\n",
       " 'note',\n",
       " 'tt',\n",
       " 'shot',\n",
       " 'hours',\n",
       " 'c',\n",
       " 'book',\n",
       " 'walk',\n",
       " 'le',\n",
       " 'order',\n",
       " 'talked',\n",
       " 'iraq',\n",
       " 't',\n",
       " 'job',\n",
       " 'parents',\n",
       " 'seeing',\n",
       " 'road',\n",
       " 'men',\n",
       " 'taking',\n",
       " 'bill',\n",
       " 'hey',\n",
       " 'saturday',\n",
       " 'remember',\n",
       " 'party',\n",
       " 'walking',\n",
       " 'share',\n",
       " 'brother']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qui riprendiamo le parole discriminanti che abbiamo estratto dal train nellle celle sopra\n",
    "parole_discriminanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_variables(post):\n",
    "    var_dict = {}\n",
    "    for parola in parole_discriminanti:\n",
    "        nome_variabile = f'has_{parola}'\n",
    "        var_dict[nome_variabile] = 1 if parola in post else 0\n",
    "    return pd.Series(var_dict)\n",
    "\n",
    "test_parole = test['post'].apply(words_variables)\n",
    "test = pd.concat([test, test_parole], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>clean_post</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_class</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_emoticon</th>\n",
       "      <th>has_punctuation</th>\n",
       "      <th>has_hope</th>\n",
       "      <th>has_hehe</th>\n",
       "      <th>has_weeks</th>\n",
       "      <th>...</th>\n",
       "      <th>has_men</th>\n",
       "      <th>has_taking</th>\n",
       "      <th>has_bill</th>\n",
       "      <th>has_hey</th>\n",
       "      <th>has_saturday</th>\n",
       "      <th>has_remember</th>\n",
       "      <th>has_party</th>\n",
       "      <th>has_walking</th>\n",
       "      <th>has_share</th>\n",
       "      <th>has_brother</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thabo admits defeat on quiet diplomacy  Mbeki ...</td>\n",
       "      <td>ooh shiny new commenting</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brainbench welcomes its 5 millionth subscriber...</td>\n",
       "      <td>today parade suked wasnt bad band year battle ...</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Even though the air in Jerusalem is dry, it is...</td>\n",
       "      <td>know anymore concerned everyday want bold face...</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there's nothing else more embarassing in life ...</td>\n",
       "      <td>roof sunset posted paul</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today I had a glass artist over for a firing. ...</td>\n",
       "      <td>god love nanny absolutely greatest woman earth...</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131698</th>\n",
       "      <td>em- i hope you go for the other job.  it sound...</td>\n",
       "      <td>cali ok ti ame missed stevi day got wasn t abl...</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131699</th>\n",
       "      <td>BLOOD PRESSURE IS REALLY REALLY HIGH, I HAD TO...</td>\n",
       "      <td>conference building</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131700</th>\n",
       "      <td>I was hoping for good news but there is no new...</td>\n",
       "      <td>dislaimer pictures shown taken heavy heart sto...</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131701</th>\n",
       "      <td>After the slow week we have had, and a couple ...</td>\n",
       "      <td>patio garden</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131702</th>\n",
       "      <td>urlLink Arafat's offices in Gaza have been bur...</td>\n",
       "      <td>day find amazing journey self discovery nice w...</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131703 rows × 546 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     post  \\\n",
       "0       Thabo admits defeat on quiet diplomacy  Mbeki ...   \n",
       "1       Brainbench welcomes its 5 millionth subscriber...   \n",
       "2       Even though the air in Jerusalem is dry, it is...   \n",
       "3       there's nothing else more embarassing in life ...   \n",
       "4       Today I had a glass artist over for a firing. ...   \n",
       "...                                                   ...   \n",
       "131698  em- i hope you go for the other job.  it sound...   \n",
       "131699  BLOOD PRESSURE IS REALLY REALLY HIGH, I HAD TO...   \n",
       "131700  I was hoping for good news but there is no new...   \n",
       "131701  After the slow week we have had, and a couple ...   \n",
       "131702  urlLink Arafat's offices in Gaza have been bur...   \n",
       "\n",
       "                                               clean_post  gender  age_class  \\\n",
       "0                                ooh shiny new commenting    male          1   \n",
       "1       today parade suked wasnt bad band year battle ...    male          1   \n",
       "2       know anymore concerned everyday want bold face...  female          1   \n",
       "3                                 roof sunset posted paul  female          1   \n",
       "4       god love nanny absolutely greatest woman earth...  female          2   \n",
       "...                                                   ...     ...        ...   \n",
       "131698  cali ok ti ame missed stevi day got wasn t abl...  female          1   \n",
       "131699                                conference building  female          1   \n",
       "131700  dislaimer pictures shown taken heavy heart sto...  female          1   \n",
       "131701                                       patio garden    male          2   \n",
       "131702  day find amazing journey self discovery nice w...    male          2   \n",
       "\n",
       "        word_count  has_emoticon  has_punctuation  has_hope  has_hehe  \\\n",
       "0              185             0                1         0         0   \n",
       "1              145             0                1         0         0   \n",
       "2               45             0                0         0         0   \n",
       "3              158             0                1         0         0   \n",
       "4              208             0                0         0         0   \n",
       "...            ...           ...              ...       ...       ...   \n",
       "131698          24             1                0         1         0   \n",
       "131699          57             0                0         0         0   \n",
       "131700          36             0                0         0         0   \n",
       "131701         261             0                0         0         0   \n",
       "131702         271             0                1         0         0   \n",
       "\n",
       "        has_weeks  ...  has_men  has_taking  has_bill  has_hey  has_saturday  \\\n",
       "0               0  ...        1           0         0        0             0   \n",
       "1               0  ...        1           1         0        1             0   \n",
       "2               0  ...        0           0         0        0             0   \n",
       "3               0  ...        0           0         0        1             0   \n",
       "4               0  ...        0           0         0        0             0   \n",
       "...           ...  ...      ...         ...       ...      ...           ...   \n",
       "131698          0  ...        0           0         0        0             0   \n",
       "131699          0  ...        0           0         0        0             0   \n",
       "131700          0  ...        0           0         0        0             0   \n",
       "131701          0  ...        1           0         0        0             0   \n",
       "131702          0  ...        1           1         0        1             0   \n",
       "\n",
       "        has_remember  has_party  has_walking  has_share  has_brother  \n",
       "0                  0          0            0          0            0  \n",
       "1                  0          0            0          1            0  \n",
       "2                  0          0            0          0            0  \n",
       "3                  0          0            0          0            0  \n",
       "4                  0          0            0          0            0  \n",
       "...              ...        ...          ...        ...          ...  \n",
       "131698             0          0            0          0            0  \n",
       "131699             0          0            0          0            0  \n",
       "131700             0          0            0          0            0  \n",
       "131701             0          0            0          0            0  \n",
       "131702             0          0            0          0            0  \n",
       "\n",
       "[131703 rows x 546 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['has_URL'] = test['post'].str.contains('urlLink').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_object(test, 'test_features_paper.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
